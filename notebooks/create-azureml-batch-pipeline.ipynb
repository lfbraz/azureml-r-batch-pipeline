{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a batch pipeline to execute R model with a change-based trigger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Azure ML SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "# check core SDK version number\n",
    "print(\"Azure ML SDK Version: \", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Azure ML Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "ws = Workspace.from_config()\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Compute Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# choose a name for your cluster\n",
    "cluster_name = \"cpu-cluster\"\n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing compute target')\n",
    "except ComputeTargetException:\n",
    "    print('Compute target not found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the environment\n",
    "Using a `DockerFile` we got from this [link](https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-commandstep-r.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "import os\n",
    "\n",
    "env = Environment.from_dockerfile(name='r_env', dockerfile='Dockerfile')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the Pipeline Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import ScriptRunConfig, Datastore\n",
    "from azureml.pipeline.core import PipelineParameter\n",
    "from azureml.data.datapath import DataPath, DataPathComputeBinding\n",
    "from azureml.pipeline.steps import CommandStep, PythonScriptStep\n",
    "from azureml.data import OutputFileDatasetConfig\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "\n",
    "source_directory = '.'\n",
    "\n",
    "# Get the Data Store\n",
    "datastore = Datastore.get(workspace=ws, datastore_name=\"<your-data-store>\")\n",
    "datastore_output = ws.get_default_datastore()\n",
    "\n",
    "# RDS Input Data parameter\n",
    "input_rds_datapath = DataPath(datastore=datastore, path_on_datastore='r-pipeline-data/rds/accidents.Rd')\n",
    "input_rds_data_pipeline_param = (PipelineParameter(name=\"input_rds_data\", default_value=input_rds_datapath),\n",
    "                             DataPathComputeBinding(mode='mount'))\n",
    "\n",
    "# MP3 Input Data parameter\n",
    "input_mp3_datapath = DataPath(datastore=datastore, path_on_datastore='r-pipeline-data/mp3')\n",
    "input_mp3_data_pipeline_param = (PipelineParameter(name=\"input_mp3_data\", default_value=input_mp3_datapath),\n",
    "                             DataPathComputeBinding(mode='mount'))\n",
    "\n",
    "from azureml.data import OutputFileDatasetConfig\n",
    "\n",
    "output_data = OutputFileDatasetConfig(name=\"output_data\", \n",
    "                                      destination=(datastore_output, \"output_data/{run-id}/{output-name}\")).as_upload()\n",
    "\n",
    "train_config = ScriptRunConfig(source_directory=source_directory,\n",
    "                            command=['Rscript Pipeline-Test.R '\n",
    "                                     '--input_rds_data', input_rds_data_pipeline_param,\n",
    "                                     '--input_mp3_data', input_mp3_data_pipeline_param,\n",
    "                                     '--output_data', output_data\n",
    "                                    ],\n",
    "                            compute_target=compute_target,\n",
    "                            environment=env)\n",
    "\n",
    "# R Model\n",
    "train = CommandStep(name='train', \n",
    "                    runconfig=train_config, \n",
    "                    allow_reuse=False, \n",
    "                    inputs=[input_rds_data_pipeline_param, input_mp3_data_pipeline_param],\n",
    "                    outputs=[output_data]\n",
    "                   )\n",
    "\n",
    "# Persist data (python script)\n",
    "run_config = RunConfiguration()\n",
    "run_config.environment.python.conda_dependencies = CondaDependencies.create(conda_packages=['pandas', 'pyodbc'])\n",
    "\n",
    "persist_data = PythonScriptStep(\n",
    "       script_name=\"persist_data.py\",\n",
    "       arguments=[\"--raw_data\", output_data.as_input('raw_data')],\n",
    "       inputs=[output_data],\n",
    "       compute_target=compute_target,\n",
    "       source_directory=source_directory,\n",
    "       runconfig=run_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import Pipeline\n",
    "from azureml.core import Experiment\n",
    "\n",
    "pipeline = Pipeline(workspace=ws, steps=[train, persist_data])\n",
    "\n",
    "# Parameters to tests our pipeline\n",
    "test_rds_path = DataPath(datastore=datastore, path_on_datastore='r-pipeline-data/rds/accidents.Rd')\n",
    "test_mp3_path = DataPath(datastore=datastore, path_on_datastore='r-pipeline-data/mp3/file_example_MP3_5MG_01.mp3')\n",
    "\n",
    "experiment_name = 'R-batch-scoring'\n",
    "pipeline_run = Experiment(ws, experiment_name).submit(pipeline, \n",
    "                                                      pipeline_parameters={\"input_rds_data\": test_rds_path,\n",
    "                                                                           \"input_mp3_data\": test_mp3_path \n",
    "                                                                          })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(pipeline_run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set secrets in KeyVault"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyvault = ws.get_default_keyvault()\n",
    "\n",
    "username = '<your-user-name>'\n",
    "password = '<your-password>'\n",
    "database = '<your-database>'\n",
    "server = '<your-server>'\n",
    "\n",
    "keyvault.set_secret(name=\"username\", value = username)\n",
    "keyvault.set_secret(name=\"password\", value = username)\n",
    "keyvault.set_secret(name=\"database\", value = database)\n",
    "keyvault.set_secret(name=\"server\", value = server)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods to disable all the schedules and pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_by_schedule_id(ws, schedule_id):\n",
    "    s = next(s for s in Schedule.list(ws) if s.id == schedule_id)\n",
    "    s.disable()\n",
    "    return s\n",
    "\n",
    "schedules = Schedule.list(ws)\n",
    "\n",
    "for schedule in schedules:\n",
    "    stop_by_schedule_id(ws, schedule.id)\n",
    "    \n",
    "def disable_all_pipelines(ws):\n",
    "    published_pipelines = PublishedPipeline.list(ws)\n",
    "    for published_pipeline in  published_pipelines:\n",
    "        published_pipeline.disable()\n",
    "        print(f\"{published_pipeline.name},'{published_pipeline.id}' disabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disable all the schedules and pipelines (only if necessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import PublishedPipeline\n",
    "from azureml.pipeline.core.schedule import ScheduleRecurrence, Schedule\n",
    "\n",
    "schedules = Schedule.list(ws)\n",
    "\n",
    "for schedule in schedules:\n",
    "    stop_by_schedule_id(ws, schedule.id)\n",
    "    \n",
    "disable_all_pipelines(ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Publish the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "published_pipeline = pipeline.publish(name='pipeline-batch-score',\n",
    "                                      description='R batch pipeline')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the change-based schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.data.datapath import DataPath\n",
    "\n",
    "datastore = Datastore.get(ws, datastore_name='<your-datastore>')\n",
    "\n",
    "reactive_schedule = Schedule.create(ws, \n",
    "                                    name=\"R-Schedule\", \n",
    "                                    description=\"Based on input file change.\",\n",
    "                                    pipeline_id=published_pipeline.id, \n",
    "                                    experiment_name=experiment_name, \n",
    "                                    datastore=datastore,\n",
    "                                    polling_interval=1,\n",
    "                                    data_path_parameter_name=\"input_mp3_data\",\n",
    "                                    path_on_datastore='r-pipeline-data/mp3/' \n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get schedule list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core.schedule import ScheduleRecurrence, Schedule\n",
    "\n",
    "ss = Schedule.list(ws)\n",
    "for s in ss:\n",
    "    print(s)\n",
    "    print('....')"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
